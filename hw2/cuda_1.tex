Our solution to this problem can be found in the folder \tty{hw2/jacobi} in our team's repository for this course.
Here is a brief overview.  There is only one source code file in this solution, \tty{parallel.cu},
plus a short Makefile.  We will say a few words about the six steps described in the algorithm overview. \\

In step 1, we declare, allocate and initialize memory for the field $u$ on the CPU.  
This is done with the array \tty{u} of size \tty{nx} x \tty{ny}.  
It's a standard nested for loop over \tty{i} and \tty{j}, with the action happening in this statement:
\begin{lstlisting}[style=CodeSnippet]
u[j*nx + i] = sin(i * dx) * sin(j * dy); 
\end{lstlisting}

In step 2, we allocate GPU memory for \tty{\_u}, \tty{\_u\_new}, and \tty{\_error}.
These are, respectively, the current field; the field at the next time step; 
and the error vs. the analytical solution.  
These two statements give an idea of how we allocate memory on the GPU and set it from the CPU:
\begin{lstlisting}[style=CodeSnippet]
cudaMalloc(&_u, nx*ny * sizeof(double));
cudaMemcpy(_u, u, nx*ny * sizeof(double), cudaMemcpyHostToDevice);
\end{lstlisting}

In step 3, we configure the GPU kernel, i.e. block sizes.
The variables \tty{tx} and \tty{ty} and both set to the preprocessor constant \tty{MAX\_THREADS\_DIM}.
This is currently set to 16, but is hardware dependent and could change if we compiled on different hardware.
A more sophisticated technique would be to detect the hardware and tune it accordingly, 
but we don't yet know how to do that, and it's not critical to performance here.
Once the thread counts are set, the box sizes \tty{bx} and \tty{by} are a straightforward 
calculation where we perform integer division rounding up.  Then we create \tty{dim3} structures.
These two statements give the flavor:
\begin{lstlisting}[style=CodeSnippet]
int bx = (int) ceil((double) nx / tx);
dim3 dimBlocks(tx, ty);
\end{lstlisting}

In step 4, we write the GPU kernel that advances the simulation one time step.
This is really the heart of the program.  
At the top, we compute the two indices \tty{ti} and \tty{tj} from the properties of the block.
We compute the updated value $U_{i,j}^{n+1}$ as the old value $U_{i,j}^{n}$ plus a prefactor
$\frac{D\Delta T}{\Delta x^2}$ multiplied by a finite difference stencil for the second derivative.
This statement computes the right hand term, i.e. the prefactor times the second derivative:
\begin{lstlisting}[style=CodeSnippet]
double rightTerm = pref * (_u[tj*nx + (ti+1)] + _u[tj*nx + (ti-1)] +
	_u[(tj+1)*nx + ti] +_u[(tj-1)*nx + ti] -4*_u[tj*nx + ti]);
\end{lstlisting}
This statement is wrapped inside an if and only executes when \tty{ti} and \tty{tj} are both
in the range mapping to actual data points.  This check is necessary in case
the number of grid points \tty{nx} and \tty{ny} are not integer multiplies of the thread count. \\

In step 5, we write the GPU kernel to compute the error against the analytical solution.
It's a similar for loop to step 4, with the expected statements inside it.
We compute the numerical and analytical solutions, then set the error
equal to the absolute value of their difference:
\begin{lstlisting}[style=CodeSnippet]
double discretizedValue = _u[tj*nx + ti];
double analyticalValue = sin(dx * ti)*sin(dy * tj)*exp(-2*D*t);
_error[tj*nx + ti] = abs(discretizedValue - analyticalValue);
\end{lstlisting}
Step 6, copying back to the CPU, is a one liner:
\begin{lstlisting}[style=CodeSnippet]
cudaMemcpy(u, _u, nx*ny * sizeof(double), cudaMemcpyDeviceToHost);
\end{lstlisting}

We ran this program on grid sizes that were powers of 2 in a reasonable range.
Specifically, we ran this for $n$ in 16, 32, 64, 256, 512.
All runs were done on a PC with a high end Titan V GPU.
We assembled charts and regression fits in \tty{jacobi.ipynb}.
We present the highlight below.